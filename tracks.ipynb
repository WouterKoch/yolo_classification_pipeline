{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO training pipeling\n",
    "\n",
    "Generic pipeline to train a classification model from observations with multiple images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic setup:\n",
    "\n",
    "import shutil\n",
    "import random\n",
    "import glob\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "dataset = \"tracks_indeed_actually\"\n",
    "\n",
    "taxa = [\n",
    "    \"Alces alces\",\n",
    "    \"Canis lupus\",\n",
    "    \"Capreolus capreolus\",\n",
    "    \"Lutra lutra\",\n",
    "    \"Meles meles\",\n",
    "    \"Sus scrofa\",\n",
    "    \"Vulpes vulpes\",\n",
    "]\n",
    "\n",
    "ratios = {\"train\": 0.7, \"validation\": 0.15, \"test\": 0.15}\n",
    "\n",
    "# Read the csv from the zip file\n",
    "zipfile = zipfile.ZipFile(f\"datasets/observations-482131.csv.zip\")\n",
    "inat_csv = pd.read_csv(zipfile.open(\"observations-482131.csv\"))\n",
    "indeed_csv = pd.read_csv(\"datasets/indeed.csv\")\n",
    "\n",
    "min_samples = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that we have a folder with raw data, separated into labels and were each file may or may not start with an observation before an underscore so that images from the same observation can all go to the same train/val/test set, copy all images into a data folder where the sets have been determined.\n",
    "\n",
    "The dataset folder in raw_data contains a folder per label.\n",
    "\n",
    "Files can be grouped into \"observations\" by having the same filename before the first underscore.\n",
    "\n",
    "These should be kept together in either train, validation or test set, with a ratio (specified above).\n",
    "\n",
    "For each label, make a list of the observations, and copy all files in the observation to either the train, validation or test set in `./data/[dataset]/[train|validation|test]/[label]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on an iNaturalist export, get a list in the right format\n",
    "# sort by id\n",
    "inat_csv = inat_csv.sort_values(\"id\")\n",
    "\n",
    "# drop the columns we don't need: species_guess, common_name, iconic_taxon_name, taxon_id\n",
    "inat_csv = inat_csv.drop(\n",
    "    [\"species_guess\", \"common_name\", \"iconic_taxon_name\", \"taxon_id\"], axis=1\n",
    ")\n",
    "\n",
    "inat_csv.columns = [\"observation\", \"image_url\", \"taxon\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the list [\"observation\", \"image_url\", \"taxon\"], download the images into a folder structure\n",
    "# loop over the rows\n",
    "\n",
    "import requests\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "import requests\n",
    "\n",
    "from PIL import Image\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from requests import ConnectTimeout\n",
    "from urllib3.exceptions import MaxRetryError\n",
    "\n",
    "target_folder = f\"raw_data/{dataset}/unclassified_images\"\n",
    "\n",
    "# drop all rows where the taxon does not contain a space\n",
    "inat_csv = inat_csv[inat_csv[\"taxon\"].str.contains(\" \")]\n",
    "\n",
    "# trim all taxa to the first two words\n",
    "inat_csv[\"taxon\"] = inat_csv[\"taxon\"].apply(lambda x: \" \".join(x.split()[:2]))\n",
    "\n",
    "# drop all rows where the taxon is not in the list\n",
    "inat_csv = inat_csv[inat_csv[\"taxon\"].isin(taxa)]\n",
    "\n",
    "for taxon in taxa:\n",
    "    if os.path.exists(f\"raw_data/{dataset}/{taxon}\"):\n",
    "        # throw an exception if the folder already exists\n",
    "        raise Exception(f\"Folder {taxon} exists, not downloading anything\")\n",
    "    \n",
    "\n",
    "\n",
    "def download(url):\n",
    "    try:\n",
    "        # if the url is empty or nan, skip\n",
    "        if pd.isna(url) or url == \"\":\n",
    "            return\n",
    "\n",
    "        basename = \"img_\" + url.split(\"/\")[-2]\n",
    "        out_path = os.path.join(target_folder, basename + \".jpg\")\n",
    "        if os.path.exists(out_path):\n",
    "            return\n",
    "        r = requests.get(\n",
    "            url.replace(\"medium\", \"large\"), stream=True, timeout=(5, 30), verify=False\n",
    "        )\n",
    "        if r.status_code == 200:\n",
    "            with open(out_path, \"wb\") as f:\n",
    "                r.raw.decode_content = True\n",
    "                shutil.copyfileobj(r.raw, f)\n",
    "            # print(f\"download {basename} took {time.time() - start_time} s\")\n",
    "        else:\n",
    "            print(f\"{url} failed to download\")\n",
    "    except MaxRetryError:\n",
    "        print(f\"MaxRetryError {url}\")\n",
    "    except ConnectTimeout:\n",
    "        print(f\"ConnectTimeout {url}\")\n",
    "    except:\n",
    "        print(f\"Other error with {url}\")\n",
    "\n",
    "\n",
    "os.makedirs(target_folder, exist_ok=True)\n",
    "\n",
    "pool = Pool(16)\n",
    "results = pool.map(download, inat_csv.image_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laurens made an annotation tool that separates different kinds of pictures. Use these to label the files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move each file to the folder as specified by the indeed.csv\n",
    "\n",
    "# loop through all files in the existing folders in the raw_data folder\n",
    "files = glob.glob(f\"raw_data/{dataset}/unclassified_images/img_*.jpg\")\n",
    "\n",
    "for index, file in enumerate(files):\n",
    "    basename = file.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    if \"[\" in file:\n",
    "        label = file.split(\"[\")[1].split(\"]\")[0]\n",
    "        basename = basename.split(\" [\")[0]\n",
    "    else:\n",
    "        indeed__row = indeed_csv[indeed_csv[\"uid\"] == basename]\n",
    "        label_true = indeed__row[\"label_true\"].values\n",
    "        label_predicted = indeed__row[\"label_predicted\"].values\n",
    "\n",
    "        if len(label_true) == 0 or pd.isna(label_true[0]):\n",
    "            label_true = []\n",
    "        else:\n",
    "            label_true = label_true[0].split(\",\")\n",
    "\n",
    "        if len(label_predicted) == 0 or pd.isna(label_predicted[0]):\n",
    "            label_predicted = []\n",
    "        else:\n",
    "            label_predicted = label_predicted[0].split(\",\")\n",
    "\n",
    "        if (\n",
    "            len(label_true) > 0 and not \"Track\" in label_true\n",
    "        ) or not \"Track\" in label_predicted:\n",
    "            label = \"no track\"\n",
    "        elif (\n",
    "            len(label_true) > 0 and \"Snow\" in label_true\n",
    "        ) or \"Snow\" in label_predicted:\n",
    "            label = \"snow track\"\n",
    "        else:\n",
    "            label = \"other track\"\n",
    "\n",
    "    basename_id = str(basename.split(\"_\")[-1])\n",
    "    inat_row = inat_csv[\n",
    "        inat_csv[\"image_url\"].str.contains(\"/\" + basename_id + \"/\", na=False)\n",
    "    ]\n",
    "\n",
    "    if len(inat_row) != 1:\n",
    "        print(f\"Error with {basename_id}: {len(inat_row)} rows found\")\n",
    "        continue\n",
    "\n",
    "    taxon = inat_row[\"taxon\"].values[0]\n",
    "\n",
    "    if \" \" not in taxon:\n",
    "        continue\n",
    "\n",
    "    taxon = \" \".join(taxon.split(\" \")[:2])\n",
    "\n",
    "    observation = inat_row[\"observation\"].values[0]\n",
    "\n",
    "    # make the folder if it does not exist\n",
    "    os.makedirs(f\"raw_data/{dataset}/{taxon}/{label}\", exist_ok=True)\n",
    "\n",
    "    shutil.move(\n",
    "        file,\n",
    "        f\"raw_data/{dataset}/{taxon}/{label}/{observation}_{index}.jpg\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename each file so that it has the observation id in the name and move it to the right folder\n",
    "for index, row in tqdm.tqdm(inat_csv.iterrows(), total=len(inat_csv)):\n",
    "    observation = row[\"observation\"]\n",
    "    image_url = row[\"image_url\"]\n",
    "    taxon = row[\"taxon\"]\n",
    "\n",
    "    # if the url is empty or nan, skip\n",
    "    if pd.isna(image_url) or image_url == \"\":\n",
    "        continue\n",
    "\n",
    "    # create the folder\n",
    "    os.makedirs(f\"raw_data/{dataset}/{taxon}\", exist_ok=True)\n",
    "\n",
    "    # move the image from the unclassified folder to the taxon folder\n",
    "    basename = \"img_\" + image_url.split(\"/\")[-2]\n",
    "    out_path = os.path.join(\n",
    "        f\"raw_data/{dataset}/{taxon}\", str(observation) + \"_\" + str(index) + \".jpg\"\n",
    "    )\n",
    "    source_folder = f\"raw_data/{dataset}/unclassified_images\"\n",
    "\n",
    "    if os.path.exists(out_path) or not os.path.exists(\n",
    "        os.path.join(source_folder, basename + \".jpg\")\n",
    "    ):\n",
    "        continue\n",
    "\n",
    "    shutil.move(\n",
    "        os.path.join(source_folder, basename + \".jpg\"),\n",
    "        out_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a folder per species, with subfolders containing \"no track\", \"other track\" and \"snow track\" images. File names start with the observation id for grouping.\n",
    "Files that were classified wrong can be copied to a folder with \"actually\" appended, i.e. \"snow track actually\". Such files will be ignored in their original folder and included as the actual label, without having to remove or move anything.\n",
    "\n",
    "The goal is to test different regimes, optimizing for snow track recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "subsets = {\n",
    "    \"train\": \"other track\",\n",
    "    \"validation\": \"other track\",\n",
    "    \"test\": \"snow track\"\n",
    "    }\n",
    "\n",
    "regime_name = f\"{dataset} ({subsets['train']} - {subsets['validation']} - {subsets['test']})\"\n",
    "\n",
    "if os.path.exists(f\"data/{regime_name}\"):\n",
    "    print(f\"Folder data/{regime_name} already exists, exiting\")\n",
    "else:\n",
    "    # Get the observation from the filename\n",
    "    def get_observation(filename):\n",
    "        filename = filename.split('/')[-1]\n",
    "        # if the filename contains an underscore, split on the first one\n",
    "        if '_' not in filename:\n",
    "            return filename\n",
    "        return filename.split('_')[0] + '_'\n",
    "\n",
    "\n",
    "    def decide_set(ratios=ratios):\n",
    "        r = random.random()\n",
    "        if \"train\" in ratios and r < ratios[\"train\"]:\n",
    "            return \"train\"\n",
    "        elif \"validation\" in ratios and r < ratios[\"train\"] + ratios[\"validation\"]:\n",
    "            return \"val\"\n",
    "        else:\n",
    "            return \"test\"\n",
    "    \n",
    "    # Removes all files that should have been classified as a different class, and adds all files that should have been classified as this class\n",
    "    def get_cleaned_files(taxon, label):\n",
    "        files = glob.glob('raw_data/' + dataset + '/' + taxon + '/' + label + '/*.jpg')\n",
    "        files = [file.split('/')[-1] for file in files]\n",
    "\n",
    "        remove_files = glob.glob('raw_data/' + dataset + '/' + taxon + '/* actually/*.jpg')\n",
    "        remove_files = [file.split('/')[-1] for file in remove_files]\n",
    "\n",
    "        files = [file for file in files if file not in remove_files]\n",
    "        files = ['raw_data/' + dataset + '/' + taxon + '/' + label + '/' + file for file in files]\n",
    "\n",
    "        add_files = glob.glob('raw_data/' + dataset + '/' + taxon + '/' + label + ' actually/*.jpg')\n",
    "\n",
    "        files = files + add_files\n",
    "        return files\n",
    "\n",
    "\n",
    "    # For each value in the subsets dict, sum the ratios of the corresponding label\n",
    "    labels = set(subsets.values())\n",
    "    taxa = os.listdir('raw_data/' + dataset)\n",
    "\n",
    "    adjusted_ratios = {}\n",
    "\n",
    "    for label in labels:\n",
    "        # get the keys of the subsets dict that have the right label\n",
    "        keys = [key for key, value in subsets.items() if value == label]\n",
    "        total_ratio = np.sum([ratios[key] for key in keys])\n",
    "        for key in keys:\n",
    "            adjusted_ratios[key] = ratios[key] / total_ratio\n",
    "\n",
    "        needed = np.sum([ratios[key] * min_samples for key in keys])\n",
    "\n",
    "        for taxon in taxa.copy():\n",
    "            files = get_cleaned_files(taxon, label)\n",
    "            if len(files) < needed:\n",
    "                print(f\"Taxon {taxon} has too few samples for label {label}\")\n",
    "                taxa.remove(taxon)\n",
    "\n",
    "    for label in labels:\n",
    "        # set the ratio to 0 if the label is not in the subsets dict\n",
    "        label_ratios = adjusted_ratios.copy()\n",
    "\n",
    "        for key in label_ratios.keys():\n",
    "            if subsets[key] != label:\n",
    "                label_ratios[key] = 0\n",
    "\n",
    "        for taxon in taxa:\n",
    "\n",
    "        # Get all observations for this label\n",
    "            files = get_cleaned_files(taxon, label)\n",
    "            observations = set([get_observation(filename) for filename in files])\n",
    "\n",
    "            # Copy all files starting with this observation to the correct set\n",
    "            for observation in observations:\n",
    "                destination = decide_set(label_ratios)\n",
    "                observation_files = [file for file in files if file.split(\"/\")[-1].startswith(observation)]\n",
    "\n",
    "                for file in observation_files:\n",
    "                    # create the destination folder if it doesn't exist\n",
    "                    os.makedirs('data/' + regime_name + '/' + destination + '/' + taxon, exist_ok=True)\n",
    "                    shutil.copy(file, 'data/' + regime_name + '/' + destination + '/' + taxon + '/' + os.path.basename(file))\n",
    "\n",
    "\n",
    "    print(f\"Files copied to data/{regime_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data, let's train a model!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train as train\n",
    "\n",
    "# Pick one of the following regimes to train the model on\n",
    "\n",
    "regime_name = f\"{dataset} (other track train)\"\n",
    "regime_name = f\"{dataset} (snow track train)\"\n",
    "regime_name =  f\"{dataset} (mix train)\"\n",
    "\n",
    "# train the model\n",
    "train.train(dataset, regime_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "options = [\"snow track\", \"mix\", \"other track\"]\n",
    "option_labels = [label.replace(\" track\", \"\") for label in options]\n",
    "runs = glob.glob(\"runs/classify/train*/\")\n",
    "\n",
    "dataset_index = 1\n",
    "\n",
    "for run in runs:\n",
    "    with open(os.path.join(run, \"args.yaml\")) as f:\n",
    "        metadata = f.read()\n",
    "        f.close()\n",
    "\n",
    "    lines = metadata.split(\"\\n\")\n",
    "    data_source = lines[3].split(\"(\")[1].split(\" train)\")[0]\n",
    "    data_path = metadata.split(\"\\n\")[3].split(\"data: \")[1]\n",
    "\n",
    "    for testlabel in options:\n",
    "        if os.path.exists(os.path.join(run, \"test results for \" + testlabel + \".csv\")):\n",
    "            continue\n",
    "\n",
    "        model = YOLO(os.path.join(run, \"weights/best.pt\"))  # initialize with best.pt\n",
    "\n",
    "        selectionset = \"all\"\n",
    "        if testlabel == data_source or data_source == \"mix\":\n",
    "            selectionset = \"test\"\n",
    "\n",
    "        dataset_config = f\"test_sets/{selectionset} {testlabel}.yaml\"\n",
    "\n",
    "        metrics = model.val(data=dataset_config, split=\"test\")\n",
    "\n",
    "        # save the results to a csv file\n",
    "        with open(\n",
    "            os.path.join(run, \"test results for \" + testlabel + \".csv\"), \"w\"\n",
    "        ) as f:\n",
    "            f.write(\"metric,value\\n\")\n",
    "            f.write(f\"top1,{metrics.results_dict.get('metrics/accuracy_top1')}\\n\")\n",
    "            f.write(f\"top5,{metrics.results_dict.get('metrics/accuracy_top5')}\\n\")\n",
    "            f.write(f\"dir,{str(metrics.save_dir)}\\n\")\n",
    "            f.close()\n",
    "\n",
    "        # rename the directory to include the test label\n",
    "        os.rename(\n",
    "            str(metrics.save_dir),\n",
    "            str(metrics.save_dir)\n",
    "            + \" \"\n",
    "            + str(dataset_index)\n",
    "            + \" (train \"\n",
    "            + data_source\n",
    "            + \", test \"\n",
    "            + testlabel\n",
    "            + \")\",\n",
    "        )\n",
    "        dataset_index += 1\n",
    "\n",
    "# --------------------------------------------\n",
    "# Make the accuracy plot\n",
    "\n",
    "plotdata = []\n",
    "\n",
    "for run in runs:\n",
    "    # read the results metadata yaml as a text file\n",
    "    with open(f\"{run}/args.yaml\") as f:\n",
    "        metadata = f.read()\n",
    "        f.close()\n",
    "\n",
    "    # get the 4th line of the metadata\n",
    "    lines = metadata.split(\"\\n\")\n",
    "    if len(lines) < 9:\n",
    "        continue\n",
    "\n",
    "    data_source = lines[3].split(\"(\")[1].split(\" train)\")[0]\n",
    "    yolomodel = lines[2].split(\"model: yolo\")[1].split(\"-cls.pt\")[0]\n",
    "    imgsize = lines[8].split(\"imgsz: \")[1]\n",
    "    run_number = run.split(\"/\")[-1]\n",
    "\n",
    "    run_csv = pd.read_csv(f\"{run}/results.csv\", sep=\",\")\n",
    "\n",
    "    datarow = {\n",
    "        \"run\": run_number,\n",
    "        \"data_sources\": data_source,\n",
    "        \"yolomodel\": yolomodel,\n",
    "        \"imgsize\": imgsize,\n",
    "        \"top1_accuracy\": run_csv[run_csv.columns[3]].values,\n",
    "        \"top5_accuracy\": run_csv[run_csv.columns[4]].values,\n",
    "    }\n",
    "\n",
    "    plotdata.append(datarow)\n",
    "\n",
    "\n",
    "# plot a graph of the top1_accuracies over time\n",
    "\n",
    "# prepare a plot with two x-axes, one for the training data (snow, mix, other) and one for the validation data (snow, mix, other)\n",
    "\n",
    "fig, ax = plt.subplots(layout=\"constrained\", figsize=(7, 4))\n",
    "\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Top 1 accuracy\")\n",
    "\n",
    "legend = []\n",
    "\n",
    "for plotdata_row in plotdata:\n",
    "    ax.plot(plotdata_row[\"top1_accuracy\"])\n",
    "    # add a legend with the data sources\n",
    "    legend.append(plotdata_row[\"data_sources\"].split(\" - \")[0].replace(\" track\", \"\"))\n",
    "\n",
    "ax.legend(legend)\n",
    "\n",
    "\n",
    "fig.savefig(f\"{run}/../top1_accuracy.png\")\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# Make the TEST accuracy plot\n",
    "\n",
    "plotdata = []\n",
    "\n",
    "dataset_index = 1\n",
    "\n",
    "for run in runs:\n",
    "    # read the results metadata yaml as a text file\n",
    "    with open(f\"{run}/args.yaml\") as f:\n",
    "        metadata = f.read()\n",
    "        f.close()\n",
    "\n",
    "    # get the 4th line of the metadata\n",
    "    lines = metadata.split(\"\\n\")\n",
    "    if len(lines) < 9:\n",
    "        continue\n",
    "\n",
    "    data_source = lines[3].split(\"(\")[1].split(\" train)\")[0]\n",
    "    yolomodel = lines[2].split(\"model: yolo\")[1].split(\"-cls.pt\")[0]\n",
    "    imgsize = lines[8].split(\"imgsz: \")[1]\n",
    "    run_number = run.split(\"/\")[-1]\n",
    "\n",
    "    run_csv = pd.read_csv(f\"{run}/results.csv\", sep=\",\")\n",
    "\n",
    "    for testlabel in options:\n",
    "        if os.path.exists(os.path.join(run, \"test results for \" + testlabel + \".csv\")):\n",
    "            test_results = pd.read_csv(\n",
    "                os.path.join(run, \"test results for \" + testlabel + \".csv\"), sep=\",\"\n",
    "            )\n",
    "                  \n",
    "            datarow = {\n",
    "                \"run\": run_number,\n",
    "                \"train\": data_source,\n",
    "                \"test\": testlabel,\n",
    "                \"yolomodel\": yolomodel,\n",
    "                \"imgsize\": imgsize,\n",
    "                \"top1_accuracy\": run_csv[run_csv.columns[3]].values,\n",
    "                \"top5_accuracy\": run_csv[run_csv.columns[4]].values,\n",
    "                \"top1_accuracy_test\": float(test_results[test_results[\"metric\"] == \"top1\"][\"value\"].values[0]),\n",
    "                \"top5_accuracy_test\": float(test_results[test_results[\"metric\"] == \"top5\"][\"value\"].values[0]),\n",
    "            }\n",
    "\n",
    "            plotdata.append(datarow)\n",
    "\n",
    "\n",
    "# prepare a plot with two x-axes, one for the training data (snow, mix, other) and one for the validation data (snow, mix, other)\n",
    "fig, ax = plt.subplots(layout=\"constrained\", figsize=(7, 4))\n",
    "\n",
    "\n",
    "ax.set_xticks(list(range(len(options) ** 2)), labels=option_labels * len(options))\n",
    "ax.tick_params(\"x\", length=0)\n",
    "\n",
    "ax.axhline(1 / 7, linestyle=\"--\", color=\"black\", linewidth=0.5)\n",
    "\n",
    "# label the classes:\n",
    "sec = ax.secondary_xaxis(location=0)\n",
    "\n",
    "triple_option_labels = option_labels * len(options)\n",
    "\n",
    "\n",
    "sec.set_xticks(\n",
    "    [-1] + list(range(len(options) ** 2)),\n",
    "    labels=[\"TRAIN\\n\\nTEST\"]\n",
    "    + np.concatenate(\n",
    "        [([\"\\n\\n\" + i] * len(option_labels)) for i in option_labels], axis=0\n",
    "    ).tolist(),\n",
    ")\n",
    "sec.tick_params(\"x\", length=0)\n",
    "\n",
    "# lines between the classes:\n",
    "sec2 = ax.secondary_xaxis(location=0)\n",
    "sec2.set_xticks([-0.5, 2.5, 5.5], labels=[])\n",
    "sec2.tick_params(\"x\", length=40, width=1.5)\n",
    "ax.set_xlim(-1.5, 8.5)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "\n",
    "\n",
    "for plotdata_row in plotdata:\n",
    "    x = options.index(plotdata_row[\"train\"]) + options.index(plotdata_row[\"test\"]) * 3\n",
    "    ax.plot(x, plotdata_row[\"top1_accuracy_test\"], marker=\"o\", markersize=5)\n",
    "\n",
    "\n",
    "fig.savefig(f\"{run}/../top1_test_accuracy.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
