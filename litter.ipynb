{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO training pipeling\n",
    "\n",
    "Generic pipeline to train a classification model from observations with multiple images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic setup:\n",
    "\n",
    "import shutil\n",
    "import random\n",
    "import glob\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "dataset = \"litter\"\n",
    "\n",
    "ratios = {\"train\": 0.7, \"validation\": 0.15, \"test\": 0.15}\n",
    "\n",
    "# Read the csv\n",
    "csv = pd.read_csv(\"datasets/litter.csv\")\n",
    "\n",
    "min_samples = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that we have a folder with raw data, separated into labels and were each file may or may not start with an observation before an underscore so that images from the same observation can all go to the same train/val/test set, copy all images into a data folder where the sets have been determined.\n",
    "\n",
    "The dataset folder in raw_data contains a folder per label.\n",
    "\n",
    "Files can be grouped into \"observations\" by having the same filename before the first underscore.\n",
    "\n",
    "These should be kept together in either train, validation or test set, with a ratio (specified above).\n",
    "\n",
    "For each label, make a list of the observations, and copy all files in the observation to either the train, validation or test set in `./data/[dataset]/[train|validation|test]/[label]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import os\n",
    "\n",
    "# Rename each file so that it has the observation id in the name and move it to the right folder\n",
    "for index, row in tqdm.tqdm(csv.iterrows(), total=len(csv)):\n",
    "    observation = str(row[\"object_id\"]).strip()\n",
    "    image_url = str(row[\"filename\"]).strip()\n",
    "    taxon = str(row[\"label\"]).strip()\n",
    "\n",
    "    # if the url is empty or nan, skip\n",
    "    if pd.isna(image_url) or image_url == \"\":\n",
    "        continue\n",
    "\n",
    "    # if the label is empty or nan, skip\n",
    "    if pd.isna(observation) or observation == \"\":\n",
    "        continue    \n",
    "\n",
    "    # create the folder\n",
    "    os.makedirs(f\"raw_data/{dataset}/{taxon}\", exist_ok=True)\n",
    "\n",
    "    # move the image from the unclassified folder to the taxon folder\n",
    "    out_path = os.path.join(\n",
    "        f\"raw_data/{dataset}/{taxon}\", str(observation) + \"_\" + str(index) + \".jpg\"\n",
    "    )\n",
    "    source_folder = f\"raw_data/{dataset}\"\n",
    "\n",
    "    if os.path.exists(out_path) or not os.path.exists(\n",
    "        os.path.join(source_folder, image_url)\n",
    "    ):\n",
    "        print(\"Skipping\", out_path)\n",
    "        print(\"File exists\", os.path.exists(out_path))\n",
    "        print(\"Source exists\", os.path.join(source_folder, image_url), os.path.exists(os.path.join(source_folder, image_url)))\n",
    "        continue\n",
    "\n",
    "    shutil.move(\n",
    "        os.path.join(source_folder, image_url),\n",
    "        out_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a folder per species, with subfolders containing \"no track\", \"other track\" and \"snow track\" images. File names start with the observation id for grouping.\n",
    "Files that were classified wrong can be copied to a folder with \"actually\" appended, i.e. \"snow track actually\". Such files will be ignored in their original folder and included as the actual label, without having to remove or move anything.\n",
    "\n",
    "The goal is to test different regimes, optimizing for snow track recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists(f\"data/{dataset}\"):\n",
    "    print(f\"Folder data/{dataset} already exists, exiting\")\n",
    "else:\n",
    "    # Get the observation from the filename\n",
    "    def get_observation(filename):\n",
    "        filename = filename.split(\"/\")[-1]\n",
    "        # if the filename contains an underscore, split on the first one\n",
    "        if \"_\" not in filename:\n",
    "            return filename\n",
    "        # return the filename up to the last underscore\n",
    "        return filename[: filename.rfind(\"_\")] + \"_\"\n",
    "\n",
    "    def decide_set(ratios=ratios):\n",
    "        r = random.random()\n",
    "        if \"train\" in ratios and r < ratios[\"train\"]:\n",
    "            return \"train\"\n",
    "        elif \"validation\" in ratios and r < ratios[\"train\"] + ratios[\"validation\"]:\n",
    "            return \"val\"\n",
    "        else:\n",
    "            return \"test\"\n",
    "\n",
    "    folders = os.listdir(\"raw_data/\" + dataset)\n",
    "    regime_name = dataset\n",
    "\n",
    "    for folder in folders:\n",
    "        files = os.listdir(\"raw_data/\" + dataset + \"/\" + folder)\n",
    "        path = \"raw_data/\" + dataset + \"/\" + folder\n",
    "\n",
    "        if len(files) < min_samples:\n",
    "            print(f\"Taxon {folder} has too few samples\")\n",
    "            continue\n",
    "\n",
    "        label = folder.split(\"/\")[-1]\n",
    "        observations = set([get_observation(filename) for filename in files])\n",
    "\n",
    "        for observation in observations:\n",
    "            destination = decide_set()\n",
    "            observation_files = [\n",
    "                file for file in files if file.split(\"/\")[-1].startswith(observation)\n",
    "            ]\n",
    "\n",
    "            for file in observation_files:\n",
    "                # create the destination folder if it doesn't exist\n",
    "                os.makedirs(\n",
    "                    \"data/\" + regime_name + \"/\" + destination + \"/\" + label,\n",
    "                    exist_ok=True,\n",
    "                )\n",
    "                shutil.copy(\n",
    "                    os.path.join(path,file),\n",
    "                    \"data/\"\n",
    "                    + regime_name\n",
    "                    + \"/\"\n",
    "                    + destination\n",
    "                    + \"/\"\n",
    "                    + label\n",
    "                    + \"/\"\n",
    "                    + os.path.basename(file),\n",
    "                )\n",
    "\n",
    "    print(f\"Files copied to data/{regime_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data, let's train a model!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train as train\n",
    "\n",
    "# train the model\n",
    "train.train(dataset, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "runs = glob.glob(\"runs/classify/train*/\")\n",
    "\n",
    "dataset_index = 1\n",
    "\n",
    "for run in runs:\n",
    "    with open(os.path.join(run, \"args.yaml\")) as f:\n",
    "        metadata = f.read()\n",
    "        f.close()\n",
    "\n",
    "    lines = metadata.split(\"\\n\")\n",
    "    data_path = lines[3].split(\"data: \")[1]\n",
    "\n",
    "    if os.path.exists(os.path.join(run, \"test results.csv\")):\n",
    "        continue\n",
    "\n",
    "    model = YOLO(os.path.join(run, \"weights/best.pt\"))  # initialize with best.pt\n",
    "    dataset_config = f\"test_sets/{dataset}.yaml\"\n",
    "    metrics = model.val(data=dataset_config, split=\"test\")\n",
    "\n",
    "    with open(\n",
    "        os.path.join(run, \"test results.csv\"), \"w\"\n",
    "    ) as f:\n",
    "        f.write(\"metric,value\\n\")\n",
    "        f.write(f\"top1,{metrics.results_dict.get('metrics/accuracy_top1')}\\n\")\n",
    "        f.write(f\"top5,{metrics.results_dict.get('metrics/accuracy_top5')}\\n\")\n",
    "        f.write(f\"dir,{str(metrics.save_dir)}\\n\")\n",
    "        f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
